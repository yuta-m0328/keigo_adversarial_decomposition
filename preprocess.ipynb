{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5b43f8-6818-425f-abf9-cb00602a3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from config import PreprocessConfig\n",
    "from datasets import MeaningEmbeddingSentenceStyleDataset\n",
    "from experiment import Experiment\n",
    "from settings import EXPERIMENTS_DIR\n",
    "from utils import load_pickle, save_json, load_json, load_embeddings, create_embeddings_matrix, extract_word_embeddings_style_dimensions\n",
    "from vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2be6f42-fce1-4ad6-8e85-8bde0a9ede6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6e82ce-5aba-4e86-b8eb-3153f39b3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e110aadd-be92-4006-a8e2-72e7939c6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(exp, dataset_train, dataset_val, dataset_test, vocab, style_vocab, W_emb):\n",
    "    # save_pickle((dataset_train, dataset_val, dataset_test), exp.experiment_dir.joinpath('datasets.pkl'))\n",
    "    # save_pickle((vocab, style_vocab), exp.experiment_dir.joinpath('vocabs.pkl'))\n",
    "    # save_pickle(W_emb, exp.experiment_dir.joinpath('W_emb.pkl'))\n",
    "\n",
    "    save_pickle(dataset_train, exp.experiment_dir.joinpath('datasets1.pkl'))\n",
    "    save_pickle(dataset_val, exp.experiment_dir.joinpath('datasets2.pkl'))\n",
    "    save_pickle(dataset_test, exp.experiment_dir.joinpath('datasets3.pkl'))\n",
    "    save_pickle(vocab, exp.experiment_dir.joinpath('vocabs1.pkl'))\n",
    "    save_pickle(style_vocab, exp.experiment_dir.joinpath('vocabs2.pkl'))\n",
    "    save_pickle(W_emb, exp.experiment_dir.joinpath('W_emb.pkl'))\n",
    "\n",
    "    print(f'Saved: {exp.experiment_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa58a099-2d0b-420a-91ac-18cc52c7648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset_reader(cfg):\n",
    "    dataset_reader_class = cfg.dataset_reader_class\n",
    "\n",
    "    dataset_reader_params = dataclasses.asdict(cfg)\n",
    "    dataset_reader = dataset_reader_class(**dataset_reader_params)\n",
    "\n",
    "    return dataset_reader\n",
    "\n",
    "\n",
    "def create_vocab(instances):\n",
    "    vocab = Vocab([Vocab.PAD_TOKEN, Vocab.START_TOKEN, Vocab.END_TOKEN, Vocab.UNK_TOKEN, ])\n",
    "    vocab.add_documents([inst['sentence'] for inst in instances])\n",
    "\n",
    "    style_vocab = Vocab()\n",
    "    style_vocab.add_document([inst['style'] for inst in instances])\n",
    "\n",
    "    return vocab, style_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb434dcd-566c-4e78-9c06-5a4a7de6a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_splits(cfg, instances):\n",
    "    if cfg.test_size != 0:\n",
    "        instances_train_val, instances_test = train_test_split(instances, test_size=cfg.test_size, random_state=42)\n",
    "    else:\n",
    "        instances_test = []\n",
    "        instances_train_val = instances\n",
    "\n",
    "    if cfg.val_size != 0:\n",
    "        instances_train, instances_val = train_test_split(instances_train_val, test_size=cfg.val_size, random_state=0)\n",
    "    else:\n",
    "        instances_train = []\n",
    "        instances_val = []\n",
    "\n",
    "    return instances_train, instances_val, instances_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c181c57-562e-425c-bcfb-0c9937249ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = PreprocessConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1dae8367-93fe-434a-a7c4-e0220661b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreprocessConfig(data_path=PosixPath('data/datasets/keigo/practice_500'), dataset_reader_class=<class 'datasets.KeigoDatasetReader'>, min_len=3, max_len=20, lowercase=True, word_embeddings='gensim', max_vocab_size=50000, nb_style_dims=50, nb_style_dims_sentences=50000, style_tokens_proportion=0.2, test_size=50, val_size=50)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b10abe8-f50d-4552-bc71-513fe462757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Experiment(EXPERIMENTS_DIR, cfg, prefix='preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51ca8efa-faa9-414f-8cd8-83e5955df20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd7249df-37fb-4d7e-ab08-243da1e21e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment started: preprocess.kniubegg\n"
     ]
    }
   ],
   "source": [
    "with Experiment(EXPERIMENTS_DIR, cfg, prefix='preprocess') as exp:\n",
    "    print(f'Experiment started: {exp.experiment_id}')\n",
    "\n",
    "    # # read instances\n",
    "    # dataset_reader = create_dataset_reader(exp.config)\n",
    "    # print(f'Dataset reader: {dataset_reader.__class__.__name__}')\n",
    "    # #KeigoDatasetReaderのメソッドreadを呼んでいる\n",
    "    # instances = dataset_reader.read(exp.config.data_path)\n",
    "    # print(f'Instances: {len(instances)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b82fe3-98e8-46f6-8027-dab517c513a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['今回',\n",
       "  'は',\n",
       "  '、',\n",
       "  'この',\n",
       "  '方',\n",
       "  'の',\n",
       "  '転職',\n",
       "  '先',\n",
       "  'と',\n",
       "  'お',\n",
       "  '仕事',\n",
       "  'が',\n",
       "  'でき',\n",
       "  'ない',\n",
       "  'か',\n",
       "  '、',\n",
       "  'と',\n",
       "  'いう',\n",
       "  'こと'],\n",
       " 'style': 'keigo'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9530f87d-461c-4aa9-95a7-60b346007278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/datasets/keigo/practice_500')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.config.data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de486080-e6ba-46a1-8e61-f414251d5e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.KeigoDatasetReader"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.dataset_reader_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25bc79c4-6bd1-4e4c-b52a-13685dc8de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment started: preprocess.e9llwftm\n",
      "Dataset reader: KeigoDatasetReader\n",
      "Instances: 991\n",
      "Vocab: 3884, style vocab: Vocab: 2 tokens\n",
      "Train: 891, val: 50, test: 50\n",
      "use gensim word embeddings.\n",
      "magnituideのロード完了\n"
     ]
    }
   ],
   "source": [
    "    with Experiment(EXPERIMENTS_DIR, cfg, prefix='preprocess') as exp:\n",
    "        print(f'Experiment started: {exp.experiment_id}')\n",
    "\n",
    "        # read instances\n",
    "        dataset_reader = create_dataset_reader(exp.config)\n",
    "        print(f'Dataset reader: {dataset_reader.__class__.__name__}')\n",
    "\n",
    "        instances = dataset_reader.read(exp.config.data_path)\n",
    "        print(f'Instances: {len(instances)}')\n",
    "\n",
    "        # create vocabularies\n",
    "        vocab, style_vocab = create_vocab(instances)\n",
    "        print(f'Vocab: {len(vocab)}, style vocab: {style_vocab}')\n",
    "\n",
    "        if exp.config.max_vocab_size != 0:\n",
    "            vocab.prune_vocab(exp.config.max_vocab_size)\n",
    "\n",
    "        # create splits\n",
    "        instances_train, instances_val, instances_test = create_splits(exp.config, instances)\n",
    "        print(f'Train: {len(instances_train)}, val: {len(instances_val)}, test: {len(instances_test)}')\n",
    "\n",
    "        # create embeddings\n",
    "        word_embeddings = load_embeddings(cfg)\n",
    "        print(\"magnituideのロード完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecfd8049-4eba-4cf5-8017-10a933ab77ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix created\n",
      "Styles instances: [444, 447]\n",
      "Styles means: [(300,), (300,)]\n",
      "Style dimensions: (50,)\n"
     ]
    }
   ],
   "source": [
    "W_emb = create_embeddings_matrix(word_embeddings, vocab)\n",
    "print(\"matrix created\")\n",
    "# extract style dimensions\n",
    "style_dimensions = extract_word_embeddings_style_dimensions(cfg, instances_train, vocab, style_vocab, W_emb)\n",
    "\n",
    "# create datasets\n",
    "dataset_train = MeaningEmbeddingSentenceStyleDataset(\n",
    "    W_emb, style_dimensions, exp.config.style_tokens_proportion,\n",
    "    instances_train, vocab, style_vocab\n",
    ")\n",
    "dataset_val = MeaningEmbeddingSentenceStyleDataset(\n",
    "    W_emb, style_dimensions, exp.config.style_tokens_proportion,\n",
    "    instances_val, vocab, style_vocab\n",
    ")\n",
    "dataset_test = MeaningEmbeddingSentenceStyleDataset(\n",
    "    W_emb, style_dimensions, exp.config.style_tokens_proportion,\n",
    "    instances_test, vocab, style_vocab\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "090b9b2d-6226-4fa5-aa95-f585e0fb3231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96105a1a-5a51-4b51-8792-73796ef7a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"test.pk\",\"wb\") as f:\n",
    "    pickle.dump(dataset_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5c731d0-0cb4-43b7-8d41-8828e1dee462",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "[E111] Pickling a token is not supported, because tokens are only views of the parent Doc and can't exist on their own. A pickled token would always have to include its Doc and Vocab, which has practically no advantage over pickling the parent Doc directly. So instead of pickling the token, pickle the Doc it belongs to.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-114e850574fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.pk\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/envs/keigo_adversarial/lib/python3.6/site-packages/spacy/tokens/token.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.__reduce__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: [E111] Pickling a token is not supported, because tokens are only views of the parent Doc and can't exist on their own. A pickled token would always have to include its Doc and Vocab, which has practically no advantage over pickling the parent Doc directly. So instead of pickling the token, pickle the Doc it belongs to."
     ]
    }
   ],
   "source": [
    "with open(\"test.pk\",\"wb\") as f:\n",
    "    pickle.dump(vocab,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005537fa-c44b-4148-9358-a49bddc94eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15067b9d-392c-455c-88ec-0e898c0edfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1441aeee73334d5292844ea268137cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/482238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "keys = [key for key,value in tqdm(word_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21eab862-1352-4d30-8a76-7048de989485",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/magnitude_keys\",\"wb\") as f:\n",
    "    pickle.dump(keys,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cbc3094-daa6-4494-8d8f-5b564ba822a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/magnitude_keys\",\"rb\") as f:\n",
    "    keys = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4aa44f3-acfd-454e-80b3-dc188211a28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocess.e9llwftm'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53887429-5d6e-40de-ac16-cac4208ff33b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e6568fa619a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_test' is not defined"
     ]
    }
   ],
   "source": [
    "dir(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f2e3d04-fb57-49f8-b83e-3dd74e36466c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "[E111] Pickling a token is not supported, because tokens are only views of the parent Doc and can't exist on their own. A pickled token would always have to include its Doc and Vocab, which has practically no advantage over pickling the parent Doc directly. So instead of pickling the token, pickle the Doc it belongs to.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-808fc0fa278c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Experiment finished: {exp.experiment_id}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-677d87e5287f>\u001b[0m in \u001b[0;36msave_dataset\u001b[0;34m(exp, dataset_train, dataset_val, dataset_test, vocab, style_vocab, W_emb)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# save_pickle(W_emb, exp.experiment_dir.joinpath('W_emb.pkl'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msave_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets1.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msave_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets2.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msave_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets3.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4368b5c35510>\u001b[0m in \u001b[0;36msave_pickle\u001b[0;34m(obj, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/envs/keigo_adversarial/lib/python3.6/site-packages/spacy/tokens/token.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.__reduce__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: [E111] Pickling a token is not supported, because tokens are only views of the parent Doc and can't exist on their own. A pickled token would always have to include its Doc and Vocab, which has practically no advantage over pickling the parent Doc directly. So instead of pickling the token, pickle the Doc it belongs to."
     ]
    }
   ],
   "source": [
    "save_dataset(exp, dataset_train, dataset_val, dataset_test, vocab, style_vocab, W_emb)\n",
    "\n",
    "print(f'Experiment finished: {exp.experiment_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63bfff6d-722d-438c-b02e-e40e76485162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'あああああ'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings._key_t(\"あああああ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6dd61f8-7625-46bb-824b-3528dbc179d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 236634/482238 [00:23<00:24, 9971.26it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-50d994ffd29e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/envs/keigo_adversarial/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/envs/keigo_adversarial/lib/python3.6/site-packages/pymagnitude/__init__.py\u001b[0m in \u001b[0;36m_iter\u001b[0;34m(self, put_cache, downloader)\u001b[0m\n\u001b[1;32m   1894\u001b[0m                 \"\"\")\n\u001b[1;32m   1895\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1896\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_db_full_result_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mput_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mput_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1897\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1898\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for key,value in tqdm(word_embeddings):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47ab6f12-7766-41c8-8b76-bdd2c70e52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7231bfae-265f-45ef-bd7d-09bf2a10b649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuta_m/.pyenv/versions/3.6.10/envs/keigo_adversarial/lib/python3.6/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/envs/keigo_adversarial/lib/python3.6/site-packages/pymagnitude/__init__.py\u001b[0m in \u001b[0;36m_iter\u001b[0;34m(self, put_cache, downloader)\u001b[0m\n\u001b[1;32m   1893\u001b[0m                     \u001b[0mFROM\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmagnitude\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m                 \"\"\")\n\u001b[0;32m-> 1895\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1896\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_db_full_result_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mput_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mput_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab3924-0115-4e54-b878-3f9a80b1b361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
