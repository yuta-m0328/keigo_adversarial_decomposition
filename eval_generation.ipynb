{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from settings import EXPERIMENTS_DIR\n",
    "from experiment import Experiment\n",
    "from utils import to_device, load_weights, load_embeddings, create_embeddings_matrix\n",
    "from vocab import Vocab\n",
    "from train import create_model\n",
    "from preprocess_train import load_dataset, create_dataset_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = './train.bfxkyc9m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment.load(EXPERIMENTS_DIR, exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<experiment.Experiment at 0x7f2d1bf48400>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocess.gb54gqgr'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.config.preprocess_exp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_exp = Experiment.load(EXPERIMENTS_DIR, exp.config.preprocess_exp_id)\n",
    "# preprocess_exp = Experiment.load(EXPERIMENTS_DIR, \"preprocess.c31qq46k\")\n",
    "dataset_train, dataset_val, dataset_test, vocab, style_vocab, W_emb = load_dataset(preprocess_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reader = create_dataset_reader(preprocess_exp.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(exp.config, vocab, style_vocab, dataset_train.max_len, W_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights(model, exp.experiment_dir.joinpath('best.th'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs(instances):\n",
    "    if not isinstance(instances, list):\n",
    "        instances = [instances,]\n",
    "        \n",
    "    if not isinstance(instances[0], dict):\n",
    "        sentences = [\n",
    "            dataset_reader.preprocess_sentence(dataset_reader.spacy( dataset_reader.clean_sentence(sent)))\n",
    "            for sent in instances\n",
    "        ]\n",
    "        \n",
    "        style = list(style_vocab.token2id.keys())[0]\n",
    "        instances = [\n",
    "            {\n",
    "                'sentence': sent,\n",
    "                'style': style,\n",
    "            }\n",
    "            for sent in sentences\n",
    "        ]\n",
    "        \n",
    "        for inst in instances:\n",
    "            inst_encoded = dataset_train.encode_instance(inst)\n",
    "            inst.update(inst_encoded)            \n",
    "    \n",
    "    \n",
    "    instances = [\n",
    "        {\n",
    "            'sentence': inst['sentence_enc'],\n",
    "            'style': inst['style_enc'],\n",
    "        } \n",
    "        for inst in instances\n",
    "    ]\n",
    "    \n",
    "    instances = default_collate(instances)\n",
    "    instances = to_device(instances)      \n",
    "    \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(outputs):\n",
    "    predicted_indices = outputs[\"predictions\"]\n",
    "    end_idx = vocab[Vocab.END_TOKEN]\n",
    "    \n",
    "    if not isinstance(predicted_indices, np.ndarray):\n",
    "        predicted_indices = predicted_indices.detach().cpu().numpy()\n",
    "\n",
    "    all_predicted_tokens = []\n",
    "    for indices in predicted_indices:\n",
    "        indices = list(indices)\n",
    "\n",
    "        # Collect indices till the first end_symbol\n",
    "        if end_idx in indices:\n",
    "            indices = indices[:indices.index(end_idx)]\n",
    "\n",
    "        predicted_tokens = [vocab.id2token[x] for x in indices]\n",
    "        all_predicted_tokens.append(predicted_tokens)\n",
    "        \n",
    "    return all_predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =  ' '.join(dataset_val.instances[1]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'） 平成 24 年 7 月 、 入管 法 が 変わり ます ！ 詳しく は 、 こちら へ 。'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = create_inputs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_styles = list(style_vocab.token2id.keys()) #['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keigo', 'normal']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences0 = [s for s in dataset_val.instances if s['style'] == possible_styles[0]]\n",
    "sentences1 = [s for s in dataset_val.instances if s['style'] == possible_styles[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 「 スマートフォン を ご 利用 の 皆 さま は ぜひ ご 活用 いただけれ ば 」 と 同局 担当 者\n",
      "27 それ で は 、 「 お 招き 頂き まし て ありがとう ござい ます 」 、 「 喜ん で 出席\n",
      "2 この 証拠 は 、 お 客 様 の ベスト 1 日 ツアー 1 日 ツアー ベスト を 支払っ て\n",
      "34 クリエイター 検定 の 学習 に 要求 さ れる こと は 才能 より も これ が 楽しい と 思う こと\n",
      "24 その こと を お 手伝い する の が 当科 の 役目 です 。\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.choice(np.arange(len(sentences0)), 5):\n",
    "    print(i, ' '.join(sentences0[i]['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 あなた たち の よう に 、 この 時期 おいしい もの も 多い し 、 つい 食べ 過ぎ て 太っ\n",
      "6 今回 は 、 この 方 の 転職 先 と お 仕事 が でき ない か 、 と いう こと\n",
      "10 で も 、 私 って ば 自分 で 自分 の 家計 を 動かし て いる 意識 が 低い の\n",
      "5 日本 軍 に 勝利 し た もの の 、 イギリス は 独立 を 許さ ず 、 再び イギリス 領\n",
      "4 はじめて 取得 し た 人 は 、 「 モナコ グランプリ に 参戦 する 」 でし た 。\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.choice(np.arange(len(sentences1)), 5):\n",
    "    print(i, ' '.join(sentences1[i]['sentence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "target0 = np.random.choice(np.arange(len(sentences0)))\n",
    "target1 = np.random.choice(np.arange(len(sentences1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多少 時間 を もっ て 登れ ば どなた に も 登れ ます 。\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(sentences0[target0]['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今 ある 安 万年 筆 と し て は これ 、 かなり いい ほう か と 。\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(sentences1[target1]['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = create_inputs([\n",
    "    sentences0[target0],\n",
    "    sentences1[target1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hidden = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hidden['style_hidden'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hidden['meaning_hidden'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_decoded = model.decode(z_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentences = get_sentences(original_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(original_sentences[0]))\n",
    "print(' '.join(original_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hidden_swapped = {\n",
    "    'meaning_hidden': torch.stack([\n",
    "        z_hidden['meaning_hidden'][0].clone(),\n",
    "        z_hidden['meaning_hidden'][1].clone(),        \n",
    "    ], dim=0),\n",
    "    'style_hidden': torch.stack([\n",
    "        z_hidden['style_hidden'][1].clone(),\n",
    "        z_hidden['style_hidden'][0].clone(),        \n",
    "    ], dim=0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaped_decoded = model.decode(z_hidden_swapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaped_sentences = get_sentences(swaped_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "お の の の の の に て 、 、 、 の の の の の の\n",
      "お の の の の の に て 、 、 、 の の の の の の\n",
      "\n",
      "お の の の の の に て 、 、 、 の の の の の の\n",
      "お の の の の の に て 、 、 、 の の の の の の\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(original_sentences[0]))\n",
    "print(' '.join(original_sentences[1]))\n",
    "print()\n",
    "print(' '.join(swaped_sentences[0]))\n",
    "print(' '.join(swaped_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
